{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Single Layer Neural Network\n",
    "\n",
    "- 딥러닝 알고리즘의 가장 기본이 되는 인공신경망(artificial neural network, ANN) 그 중에서도 single-layer neural network 모델을 구현해보자.\n",
    "\n",
    "#### 크게 세가지 방식\n",
    "- 1) Random Search\n",
    "- 2) h-step Search\n",
    "- 3) Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "- 0~1 사이에 균등분포를 가지는 100개의 샘플을 각각 x1, x2로 만든다.\n",
    "- 0.3 * x1 + 0.5 * x2 + 0.1의 공식을 가지는 y를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.71344839, 0.66450119, 0.53485494, 0.26892626, 0.43587185,\n",
       "       0.95140088, 0.19655521, 0.46881292, 0.39076559, 0.21865436])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x1.shape)\n",
    "x1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99341046, 0.99772426, 0.45520151, 0.12987227, 0.59119058,\n",
       "       0.77094153, 0.83107612, 0.93524068, 0.82563196, 0.33778139])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x2.shape)\n",
    "x2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.81073975, 0.79821249, 0.48805724, 0.24561401, 0.52635685,\n",
       "       0.77089103, 0.57450462, 0.70826422, 0.63004566, 0.334487  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0.3 * x1 + 0.5 * x2 + 0.1\n",
    "\n",
    "print(y.shape)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First idea: Random Search\n",
    "- 랜덤으로 weight값(w1, w2)과 바이어스(bias) 값(b)을 만들어 최소한의 error를 만들어 내는 값을 여러번의 시도를 통해 찾아낸다.\n",
    "- **답(목표error값)을 찾을 때까지 w1, w2, b을 랜덤으로 찾기 때문에 1번만에 끝나거나 10000을 다돌려도 못찾을 수 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 w1 = 0.57986, w2 = 0.05484, b = 0.55502, error = 0.34047\n",
      "   3 w1 = 0.67128, w2 = 0.08402, b = 0.33361, error = 0.20212\n",
      "   8 w1 = 0.00712, w2 = 0.77407, b = 0.15153, error = 0.10809\n",
      "  26 w1 = 0.19931, w2 = 0.41073, b = 0.21938, error = 0.03755\n",
      "  95 w1 = 0.21394, w2 = 0.47162, b = 0.14996, error = 0.02235\n",
      " 661 w1 = 0.32373, w2 = 0.43404, b = 0.11620, error = 0.02009\n",
      "7700 w1 = 0.29914, w2 = 0.51221, b = 0.10214, error = 0.00858\n",
      "------------------------------------------------------------\n",
      "7700 w1 = 0.29914, w2 = 0.51221, b = 0.10214, error = 0.00858\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10000\n",
    "\n",
    "best_error = np.inf\n",
    "best_epoch = None\n",
    "best_w1 = None\n",
    "best_w2 = None\n",
    "best_b = None\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "    b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "    y_predict = x1 * w1 + x2 * w2 + b\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_epoch = epoch\n",
    "        best_w1 = w1\n",
    "        best_w2 = w2\n",
    "        best_b = b\n",
    "\n",
    "        print(\"{0:4} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(best_epoch, best_w1, best_w2, best_b, best_error))\n",
    "\n",
    "print(\"----\" * 15)\n",
    "print(\"{0:4} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(best_epoch, best_w1, best_w2, best_b, best_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2 - h-step Search\n",
    "- w1, w2, b 값을 한번 랜덤으로 만든다.\n",
    "- 첫번째 error 값을 구한후, 정해진 스텝수(h)만큼 w1을 +,-하며 error가 줄어드는 방향으로 업데이트 한다.\n",
    "- w2와 b도 동일한 방법으로 첫번째 error와 비교하여 error가 줄어드는 방향으로 업데이트 한다.\n",
    "- **스텝수(h)가 적절하지 않으면 여러번 돌려도 error를 줄일 수 없다.**\n",
    "- **Random Search와 마찬가지로 처음생성되는 w1, w2, b값과 스텝수의 따라 1번만에 목표 error값에 도달하는 값을 찾거나 10000번이 지나도 못찾을 수도 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 w1 = 0.30770, w2 = 0.49417, b = 0.10148, error = 0.00282\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10000\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "h = 0.01\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = x1 * w1 + x2 * w2 + b\n",
    "    current_error = np.abs(y_predict - y).mean()\n",
    "\n",
    "    if current_error < 0.005:\n",
    "        break\n",
    "\n",
    "    y_predict = x1 * (w1 + h) + x2 * w2 + b\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    if h_plus_error < current_error:\n",
    "        w1 = w1 + h\n",
    "    else:\n",
    "        y_predict = x1 * (w1 - h) + x2 * w2 + b\n",
    "        h_minus_error = np.abs(y_predict - y).mean()\n",
    "        if h_minus_error < current_error:\n",
    "            w1 = w1 - h\n",
    "            \n",
    "    y_predict = x1 * w1 + x2 * (w2 + h) + b\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    if h_plus_error < current_error:\n",
    "        w2 = w2 + h\n",
    "    else:\n",
    "        y_predict = x1 * w1 + x2 * (w2 - h) + b\n",
    "        h_minus_error = np.abs(y_predict - y).mean()\n",
    "        if h_minus_error < current_error:\n",
    "            w2 = w2 - h\n",
    "\n",
    "    y_predict = x1 * w1 + x2 * w2 + (b + h)\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    if h_plus_error < current_error:\n",
    "        b = b + h\n",
    "    else:\n",
    "        y_predict = x1 * w1 + x2 * w2 + (b - h)\n",
    "        h_minus_error = np.abs(y_predict - y).mean()\n",
    "        if h_minus_error < current_error:\n",
    "            b = b - h\n",
    "\n",
    "print(\"{0} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, current_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Idea - Gradient Descent\n",
    "- **dError/dWeight한 값으로 weight를 업데이트 해주는 방법이다.** (에러를 각각 업데이트할 weight로 편미분 하면 w의 업데이트 방향설정이 가능하다.)\n",
    "- learning rate는 편미분한 값의 방향을 정한후 학습할 크기를 말한다. (적절한 값이 필요하다.)\n",
    "- 경사감소법은 기울기를 구해 error를 줄이는 방향을 설정하기 때문에 정답에 근사하게 접근할 수 있다.(위 두방법보다 업데이트의 방향성이 보장된다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w1 = 0.48173, w2 = 0.25863, b = -0.21823, error = 0.39827\n",
      "10 w1 = 0.38164, w2 = 0.43380, b = -0.03058, error = 0.13723\n",
      "20 w1 = 0.33821, w2 = 0.48555, b = 0.04449, error = 0.04710\n",
      "30 w1 = 0.31844, w2 = 0.49919, b = 0.07556, error = 0.01610\n",
      "40 w1 = 0.30909, w2 = 0.50182, b = 0.08888, error = 0.00564\n",
      "50 w1 = 0.30454, w2 = 0.50169, b = 0.09480, error = 0.00215\n",
      "------------------------------------------------------------\n",
      "60 w1 = 0.30291, w2 = 0.50171, b = 0.09822, error = 0.00091\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "learning_rate = 1.2\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    if error < 0.001:\n",
    "        break\n",
    "\n",
    "    w1 = w1 - learning_rate * ((y_predict - y) * x1).mean()\n",
    "    w2 = w2 - learning_rate * ((y_predict - y) * x2).mean()\n",
    "    b = b - learning_rate * (y_predict - y).mean()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"{0:2} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, error))\n",
    "    \n",
    "print(\"----\" * 15)\n",
    "print(\"{0:2} w1 = {1:.5f}, w2 = {2:.5f}, b = {3:.5f}, error = {4:.5f}\".format(epoch, w1, w2, b, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
